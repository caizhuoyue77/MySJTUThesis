\chapter{知识图谱与大语言模型互增益的API编排方法}

\section{引言}
\label{sec:intro}
\subsection{基于深度优先遍历的动态搜索算法}
\subsubsection{基于深度优先遍历的寻路算法}
\subsubsection{基于反思机制的回溯算法}
\subsubsection{基于API选择路径的动态权值调整}

\subsection{工具调用路径长短期记忆框架}

\subsubsection{长短期记忆框架概述}

本小节提出了关于增强模型规划和推理准确性的长短期记忆框架。该框架主要分为短期记忆和长期记忆两个部分。短期记忆部分指的是当模型在图上动态推理时存储的每个步骤的记录，主要聚焦于以什么数据格式来存储推理步骤，以及记录哪些有用的记忆信息：在图上前进、回溯还有大模型的思考过程等。长期记忆部分主要有记忆存储、召回和利用三个部分，围绕着如何进一步利用过去成功推理的经验来辅助后续的推理和规划过程。

\subsubsection{短期记忆}

短期记忆指的是模型在图上进行不断推理时保存的一些状态信息，这些信息包括：用户的任务、当前遍历到的节点、历史遍历的节点、轨迹路径信息等。在遍历的过程中，我们会动态地更新和维护短期记忆存储的内容，来辅助模型进行推理和规划。在短期记忆中，我们一般将全部的信息存储在内存中，然后直接构建提示词添加到大语言模型的上下文中，让模型能够感知到当前的状态和环境。

\subsubsection{长期记忆}

长期记忆是与短期记忆相对的概念，长期记忆会固定地存在数据库中，并且随着调用次数增多而积累。长期记忆记录的是最终形成的工具调用链以及结果，每次在系统进行推理时都会从长期记忆库中进行搜索，从而利用历史经验知识来辅助模型的推理。

长期记忆的具体实现方式如下，包括记忆的添加、删除、修改和查询四个部分。

\begin{enumerate}
    \item \textbf{记忆添加} \\
    记忆增加是该框架中最基础的功能。首先，除了初始记忆批量添加的阶段，其他的记忆添加都是在生成了新的推理路径时进行。我们先对新生成的推理路径进行筛选，通过一个评判器来对推理路径判断质量。如果评判器认定为“失败”或“不确定”，那么就舍弃当前的推理路径。这一步的判别是为了保证记忆的质量，不存储有失败风险的路径在记忆中。随后，我们将用户的需求与推理路径形成一个二元组，将用户需求输入向量模型转为嵌入向量，将推理路径作为文本格式存储，方便计算和检索记忆。

    \item \textbf{记忆修改} \\
    对于同一个任务，系统可能会生成不同的推理路径。在这种情况下，我们会用新的记忆来替换旧的记忆，对记忆做出及时的更新。在记忆修改时，我们只需要对推理路径部分进行修改，并保存更新后的路径在数据库中，而不需要对用户需求的向量部分进行修改。

    \item \textbf{记忆删除} \\
    由于工具节点的状态是随时变化的，比如开发者停止维护了某个工具，或者是某个工具 API 出现故障等等，因此有的时候我们会删除一些工具节点或修改节点状态为“失效”。在这种情况下，我们除了在工具图谱上进行修改，也应该同时删除有关的记忆，避免对模型造成困扰。记忆删除的逻辑较为清晰，即当某工具被删除或失效时，在记忆库中匹配所有包含该工具的路径并删除对应的记忆。

    \item \textbf{记忆查询} \\
    在有新的用户需求时，系统会首先将用户需求转为向量形式，然后在向量数据库中通过相似度检索算法查找到与当前需求最相似的 K 个历史需求以及对应的工具轨迹，即搜索得到的记忆。
\end{enumerate}

\subsubsection{记忆搜索与召回}
\subsubsection{记忆利用}
（可以做消融实验）

\section{真实工具调用模拟}
\label{sec:real_tool_simulation}

\section{实验与评估}
\subsection{测试数据集}
\label{subsec:test_dataset}

ToolBench。ToolBench\cite{Qin2023}是一个公开的针对工具调用的数据集，其中包含了来自49个类别的16464个真实世界的API工具的推理轨迹数据。该数据集包括三个部分，三个子数据集的难度逐级上升：G1数据集，其中目标任务所需的API都在同一个工具组；G2数据集，其中目标任务所需的API在同一个类别但是属于不同的工具组；G3数据集，其中目标任务所需的API会跨越不同类别。为了测试各个难度等级上的能力，本工作从三个类别分别抽取了350，350和300条数据构建测试集。测试集一共涉及18个category的358个工具。

API-Bank。API-Bank\cite{Li2023}是另一个公开的工具调用数据集，作者们针对模型工具调用的检索、规划能力的评估精心构建了测试数据，其中包含73个API工具，并对314个工具调用进行了标注。本工作从中抽取了300条数据作为测试集。

\subsection{评估指标}
由于工具的多样性，对于同一个用户需求可以有多种工具调用路径。因此，我们无法事先对每个测试的输入标注单一的解决路径标准答案。由于人工评价较为费时费力，本文基于\cite{Tang2023}中的评估器构建了类似的评估体系，包含以下两个指标。我们的评估器使用的是目前能力最强的模型之一GPT-4，温度系数设置为0。（todo）

\begin{itemize}
    \item \textbf{通过率（Pass Rate）} \\
    通过率是计算在有限的工具执行步骤内完成了需求的比例。该指标衡量了系统工具调用最基本的执行能力。通过率的公式如下：

    \begin{equation}
        PR = \frac{ \#(\text{Solved}) }{ \#(\text{Solved}) + \#(\text{Unsolved}) }.
    \end{equation}

    \item \textbf{胜率（Win Rate）} \\
    胜率是评价两条针对同一需求生成的路径的偏好。在模型判断胜率的评估器的提示词中，我们预先定义了一组标准，其中包括：探索性、真实性、工具个数。胜率的公式如下：

    \begin{equation}
        WR = \frac{ \#(\text{Won}) }{ \#(\text{Won}) + \#(\text{Lost}) + \#(\text{Tie}) }.
    \end{equation}

\end{itemize}

同时，为了验证评估器与人类标注者的标注一致性，我们人工标注了100条通过率和100条胜率的数据。经过这200条数据，我们发现标注器在通过率上与人工标注的一致性达到了xxx（todo），在胜率上该数字达到了xxx（todo），这表明该基于大语言模型的标注器与人工标注的标准基本吻合。

\subsection{实验结果}
我们在不同基模型上测试了不同的工具编排调用方式并与我们的方法进行对比。
\begin{itemize}
    \item 基准线：其他的vanilla、cot、react的prompt等。
\end{itemize}

\subsection{实验分析}
\label{subsec:exp_analysis}

\section{本章小结}
\label{sec:summary_chap4}