\chapter{知识图谱与大语言模型互增益的API编排方法}

\section{引言}
\label{sec:intro}

\indent 在第三章中，我们介绍了基于工具调用轨迹构建的API知识图谱，包括有对调用轨迹数据的清洗、图谱的静动态构建算法、API节点召回模型训练等部分。
在本章，将会利用该图谱包含的API之间的依赖信息，集中在图谱利用和API路径编排的部分，即如何根据用户需求在大型工具图谱上进行高效的搜索和选择。
在本章主要有以下几个重要问题需要研究：
1.如何合理利用工具图谱上的依赖信息进行工具选择？
2.如何对路径选择流程进行优化，以提升整体的准确率和效率？
3.如何处理工具调用中遇到的工具调用异常、工具响应过长等问题？

\indent 针对上述问题，我们提出了一种基于工具图谱的动态寻路算法。该算法首先将用户的需求进行分析和拆解，以得到
更小的任务编排与执行单位。其次，本章提出了一种基于深度优先搜索的搜索算法，
能够实时地在图上进行搜索并选择合适的API调用路径。最后，由于API工具调用的返回结果内容较多，本着减少大语言模型推理时间与提升系统效率的考虑，
我们提出了一种响应内容压缩方法，通过让模型选择重要的字段来生成更短的响应结果，能够有效保留重要信息并提升交互效率。

\section{问题定义}

工具增强的大语言模型（LLM）通过结构化的自然语言文本与环境进行交互。我们提出了一种新的解决方案，结合了一个由1.6万个API构成的API图作为工具池，以优化API调用和任务执行的过程。具体的交互流程如下：

在接收到任务时，首先通过任务分解模块将任务分解为多个子任务，每个子任务将单独进行处理。任务分解的过程可以表示为：

\[
T_{sub} = \text{Task\_decomposer}(T) \tag{1}
\]

其中，\( T_{sub} \) 是分解后的子任务集合，\( T \) 是输入的原始任务。

对于每个子任务的API选择，采用API检索模型在API池中检索出前 \( k \) 个与子任务最相关的API作为初始节点。具体步骤可以形式化为：

\[
A_{init} = \text{API\_retriever}(T_{sub}, q) \tag{2}
\]

其中，\( A_{init} \) 表示初始节点的集合，\( T_{sub} \) 是API工具池，\( q \) 是当前子任务的查询。

随后，LLM将基于初始节点进行迭代选择新的API节点并进行调用。这个选择过程不仅考虑当前任务的需求，还会记录选择过程中的调用结果，以便后续的回溯和调整：

\[
a_s = P_\theta(C_s \cup o_s, t_s, A_{current}) \tag{3}
\]

在上式中，\( A_{current} \) 代表当前任务相关的API节点集合，而 \( a_s \) 是选择的API动作。

在所有子任务的API调用成功后，我们将每个子任务的输出和结果汇总到汇总器，最终得出一个综合的结论。汇总的过程可以表示为：

\[
R_{final} = \text{Aggregator}(R_{sub}) \tag{4}
\]

其中，\( R_{final} \) 是最终的结论，\( R_{sub} \) 是各子任务的结果集合。

为提升长期记忆能力，我们还存储过去的调用历史，并通过向量检索方法查找类似的查询和路径，从而辅助当前的决策过程。长期记忆的更新可以通过以下公式表示：

\[
M_{long} = \text{Vector\_search}(H, q) \tag{5}
\]

其中，\( M_{long} \) 表示长期记忆，\( H \) 是过去的调用历史，\( q \) 是当前查询。

此外，API调用的过程经过简单配置参数后，可以直接进行调用，并对过长的响应进行压缩，以提高响应效率。这个过程可以表示为：

\[
r_s = \text{API\_call}(a_s, \text{params}) \tag{6}
\]

在这里，\( r_s \) 表示API的响应，\( a_s \) 是所选择的API动作，\(\text{params}\) 是配置的参数。

通过这样的设计，我们的基于大语言模型Agent和图谱的API编排和调用工具能够有效地处理复杂任务，优化API的调用流程和准确性。

\section{整体框架}

图~\ref{fig:ch4-framework}展示了本章提出的基于深度优先遍历算法的动态工具编排算法的整体技术框架。

该方法由以下几个部分组成：

\begin{enumerate}
  \item 任务分解模块：任务分解模块负责将复杂、模糊的用户需求分解为多个子任务，能够充分提高系统处理复杂任务的能力。
  \item 任务编排模块：该部分包括基于深度优先遍历的动态工具搜索算法、自我反思机制和长短期记忆模块。动态搜索能够提高系统的灵活性和可用性，自我反思机制能够通过格式化的反思提升系统的准确性，长短期记忆能够通过历史记录辅助当前决策。
  \item 任务执行模块：任务执行模块负责调用工具API，并将结果进行汇总，最终输出结果。其中涉及到了API参数配置、API响应压缩模块。
\end{enumerate}

\begin{figure}[!htp]
  \vspace{1em}
  \centering
  \setlength{\abovecaptionskip}{10pt} % 控制图片和caption之间的距离
  \includegraphics[height=6cm]{../assets/ch3-1.pdf}
  \bicaption{整体框架}{Overview of the Dynamic Tool Selection Framework}
  \label{fig:ch4-framework}
\end{figure}

\section{具体实现}

\subsection{任务改写与子任务拆解模块}

由于用户的需求可能会较为模糊、笼统，或者在同一个需求语句中存在多个潜在的子任务。通过对用户提供的复杂需求进行分解、改写，并生成适合执行的具体指令，这一过程使得任务变得更加明确和易于解决。

任务分解模块的输入包括具体用户需求、子任务格式的指令、输出格式案例以及当前工具的具体分类，输出则是JSON格式的一组子任务。每个子任务都是一个独立的任务单元，包括“子任务名称”、“子任务描述”、“子任务类别”等重要信息。每个子任务都可以看作一个完整的任务进行执行。

首先，任务分解模块的核心功能是将用户提出的复杂或模糊需求拆解为可执行的子任务。许多用户在表达需求时，往往由于信息不明确或需求过于笼统，导致系统难以直接响应。例如，用户可能提出多个相关或不相关的要求，或者在一个指令中混合了不同领域的子任务。在这种情况下，大语言模型通过对用户输入的语义分析，将任务按照逻辑和类别进行分解。每一个子任务将独立处理，从而避免因任务过于复杂而导致系统误解或错误执行。

其次，改写器的作用是将分解后的子任务进一步精炼成更具体、明确的语句。这一过程借助于大语言模型强大的自然语言处理能力，通过对原始需求的语义理解和推理，将模糊的任务表达转化为更加清晰、具体的指令。例如，用户可能提出“帮我分析特斯拉的股票数据并生成趋势报告”的需求。改写器会将其分解为多个步骤：首先获取股票数据，然后分析数据中的关键趋势，最后生成一份详细报告。通过这一改写过程，系统可以更好地执行每一步任务，通过调用查询数据的工具API获得相关信息，并在获取股票数据后，通过大语言模型自身的分析功能生成分析报告。

此外，任务分解模块能够显著提升大语言模型在多工具和多类别场景中的应用能力。通过将复杂任务拆解为多个相互独立的子任务，系统能够并行执行这些子任务，并分别调用不同的工具或API来完成每一步。子任务中包含“任务类别”，通过在任务分解模块中划分好任务的类别，可以有效减少后续动态工具搜索时的搜索空间，提升搜索的准确性和速度。这种设计允许各个子任务同时进行，显著提高任务执行效率。

总的来说，基于大语言模型的任务分解模块通过分解复杂任务、改写用户需求，并动态调整任务规划，使得该系统在执行复杂任务时更加灵活、精准且高效。


% 设置颜色
\definecolor{bgcolor}{RGB}{240,240,240} % 背景颜色
\definecolor{titlecolor}{RGB}{20,20,20} % 标题背景颜色

\begin{center}
% 使用 tcolorbox 创建背景
\begin{tcolorbox}[colback=bgcolor, colframe=black, width=0.8\textwidth, boxrule=0.5mm, 
coltitle=white, colbacktitle=titlecolor, title=An Example for Response Generation with GPT-4]

% 使文字居中

\textbf{Instruction Prompt:} You are currently in the response generation stage. You are given the output of selected tools that can be potentially used to solve the sub-task. Your goal is to generate a response to the user question based on the results from tools and your internal knowledge.

\textbf{User Question:} I would like to know the value of 5 ounces of gold plus 1 million AMZN stocks in CNY.

\textbf{Return from Tool Calling:} The stock price of AMZN is 130 USD, XAU USD is 1919.7605807522993, and the exchange rate between USD and YNC is 7.24.

\textbf{Output:} \textcolor{red}{Total value in CNY: Total value = Value of gold in CNY + Value of stocks in CNY = 69,495.87 + 941,200,000 = 941,269,495.87 CNY. Therefore, the combined value of 5 ounces of gold and 1 million Amazon stocks in Chinese Yuan is approximately 941,269,495.87 CNY.}

\end{tcolorbox}

\end{center}


\subsection{基于深度优先遍历的动态搜索算法}

为了更好地利用我们构建的工具图谱，并挖掘隐藏节点关系中的知识，我们开发了一个基于深度优先遍历的寻路算法。与“思维链”（Chain-of-Thought）或ReACT方法相比，该算法的优点在于能够防止错误传播，并对整体工具行动空间进行更深入的探索。

我们提出的基于深度优先的寻路算法的流程图如下所示：

首先，由于图谱上的节点数量众多，选择初始节点是算法中非常关键的一步。在选择初始节点时，我们利用基于语义相似度的API召回器，得到与用户需求在语义上最相似的一组工具，让大语言模型进行选择，最终确定初始节点，并从该节点开始进行后续探索。

其次，我们在图上构建一棵深度优先搜索树，允许模型在图上不断选择新的节点并评估每一条推理路径。具体逻辑如下：对于每个选择的节点，我们调用该节点并获取对应的API响应内容，然后将已经调用的工具轨迹、解析后的API响应、用户需求等提供给大语言模型，让模型选择下一步操作。下一步操作可以是继续选择节点，也可以是回溯到上一节点。

如果模型选择了下一步操作，我们将获取当前节点的邻居节点及其权值，并提供给大模型智能体。对于邻居节点的选择，我们会获取到当前节点权值最大的K个节点作为下一步操作。如果邻居节点不足5个，我们将重新调用API召回器，以补全5个选择的选项。模型会根据当前状态继续迭代选择，直到结束选择或放弃。

如果模型选择回溯到上一节点，我们将在短期记忆模块中添加回溯步骤，并恢复到上一个节点的状态。将回溯步骤添加到短期记忆的原因是让模型记住在本次推理中之前采取的错误操作，避免后续重复选择不可行的工具。如果模型回溯到了初始节点，并且需要继续回溯，我们可以理解为基于当前的工具组，该任务不可行，即放弃任务执行。

\begin{algorithm}[htb]
    \caption{图谱节点选择算法}
    \label{algo:algorithm}
    \small
    \SetAlgoLined
    \KwData{用户需求 $user需求$}
    \KwResult{选择合适的工具}
  
    // 选择初始节点\;
    $initi
    alNodes \gets API召回器(user需求)$\;
    $selectedNode \gets LLM选择初始节点(initialNodes)$\;
    
    // 初始化深度优先搜索树\;
    $dfsTree \gets \text{new Tree()}$\;
    $dfsTree.addNode(selectedNode)$\;
  
    \While{true}{
      $response \gets 调用API(selectedNode)$\;
      
      // 提供信息给大语言模型\;
      $状态信息 \gets \{\text{工具轨迹}, \text{API响应}, \text{用户需求}\}$\;
      $nextAction \gets LLM选择下一步操作(状态信息)$\;
  
      \eIf{nextAction == "选择下一节点"}{
        $topNeighbors \gets 选择邻居节点(selectedNode)$\;
        \If{length(topNeighbors) < 5}{
          $topNeighbors \gets API召回器补全选择(selectedNode)$\;
        }
        $selectedNode \gets LLM选择下一个节点(topNeighbors)$\;
        $dfsTree.addNode(selectedNode)$\;
      }{
        // 回溯逻辑\;
        $短期记忆.add回溯步骤(dfsTree.currentNode())$\;
        $selectedNode \gets dfsTree.backtrack()$\;
  
        \If{selectedNode == dfsTree.root() \&\& 需要继续回溯()}{
          放弃任务()\;
          break\;
        }
      }
      
      \If{模型结束选择()}{
        break\;
      }
    }
  \end{algorithm}

\subsection{自我反思机制}

本文构建的自我反思框架会在动态寻路算法选择“放弃”或者评判器将该路径判定为“失败”的情况下被激活。在自我反思框架中，我们首先需要进行“反思”，即根据当前编排得到的工具调用路径，识别未能成功满足用户需求的原因。具体而言，当动态寻路算法的选择器决定“放弃”某条路径时，会提供该路径放弃的具体原因。这些“放弃原因”可作为反思的依据，以指导模型在后续的路径选择中作出更为合理的决策。此外，在动态寻路算法得出一个解决方案后，若经过评估器的评估发现该路径未能充分满足用户需求，则会引用评估器所提供的失败理由。评估器在评估失败时会对该路径及总结器的最终回答提供一个等级，根据失败的等级我们可以选择以下两种不同的重新寻路算法：第一种是从中间步骤开始重新寻找路径，另一种是从头开始重新寻找路径。

\begin{enumerate}
  \item \textbf{从中间步骤继续}: 在任务未完成的情况下，我们将持续记录检索过程中的每一步输入、输出及上下文信息。当路径被标记为“放弃”或被评判器认定为“失败”时，我们会重新激活该路径上的智能体，并将识别到的失败原因重新纳入历史上下文。评判器在判定“失败”时，通常会标记出它认为的第一个出现错误的节点。在重新激活智能体并进行寻路时，我们将从该节点继续，而不是从头开始。这种从中间步骤继续的策略不仅能够加快寻路速度，减少大语言模型的调用次数，还能充分利用先前成功调用的经验，从而提升决策的准确性。
  \item \textbf{重新寻路}: 在评判器认为路径“完全失败”时，我们需要从头开始重新生成整条路径。评判器会识别路径初始节点中与用户查询无关的工具名称作为反思的一部分。为了提高系统的整体效率，我们会首先从初始工具节点中移除这些无关的工具，避免大模型受到这些噪声的影响，从而选择无关的工具进行调用，导致后续调用出错。通过这一清理过程，我们能够有效减少噪声工具的影响，确保后续搜索的准确性。尽管本文提出的基于深度优先遍历的算法可以通过回溯来避免错误传播的问题，但我们也应尽可能地避免选择错误和无关的工具，这样会增加寻路时间，也会使算法面临更多的不稳定性。\par
  接下来，我们将会在经过噪声清理的工具组中重新开始选择下一节点并组成路径。这种从头开始的自我反思允许算法在一个更加简洁与优化的初始条件下进行搜索，从而提升工具调用路径的质量与响应速度。
\end{enumerate}
 
该自我反思机制可以反复应用，直至满足终止条件为止。这种持续的反思过程确保了对问题的深入理解和逐步优化，有助于形成更加有效的调用路径。

综上所述，这两种反思策略——从中间步骤继续优化和从头开始的寻路——的结合使用，能够在处理用户需求未满足的情况下，提供更高的灵活性与效率。通过不断的反思与优化，系统将逐步提升其在动态环境中的适应能力，确保用户体验的持续改进。

\subsection{工具调用路径长短期记忆框架}

\subsubsection{长短期记忆框架概述}

本小节提出了关于增强模型规划和推理准确性的长短期记忆框架。该框架主要分为短期记忆和长期记忆两个部分。短期记忆部分指的是当模型在图上动态推理时存储的每个步骤的记录，主要聚焦于以什么数据格式来存储推理步骤，以及记录哪些有用的记忆信息：在图上前进、回溯还有大模型的思考过程等。长期记忆部分主要有记忆存储、召回和利用三个部分，围绕着如何进一步利用过去成功推理的经验来辅助后续的推理和规划过程。

\subsubsection{短期记忆}

短期记忆指的是模型在图上进行不断推理时保存的一些状态信息，这些信息包括：用户的任务、当前遍历到的节点、历史遍历的节点、轨迹路径信息等。在遍历的过程中，我们会动态地更新和维护短期记忆存储的内容，来辅助模型进行推理和规划。在短期记忆中，我们一般将全部的信息存储在内存中，然后直接构建提示词添加到大语言模型的上下文中，让模型能够感知到当前的状态和环境。

\subsubsection{长期记忆}

长期记忆与短期记忆相对，是固定存储在数据库中的信息，且会随着调用次数的增加不断积累。它记录最终形成的工具调用链和结果，每次推理时，系统都会从长期记忆库中搜索，利用历史经验辅助模型推理。

长期记忆模块基于检索增强生成（Retrieval-Augmented Generation, RAG）逻辑，通过将历史上成功的<用户需求，工具轨迹路径>转为向量存储。新需求通过向量模型转为嵌入向量，并与数据库中的向量进行相似度计算。系统会根据相似度排序，检索出与当前需求最相似的 K 个历史需求及其工具轨迹。最终，这些<用户需求，工具轨迹路径>的二元组会被加入大语言模型的上下文中，辅助推理与规划。

\subsection{工具调用模块}
\label{sec:real_tool_simulation}

\subsubsection{整体逻辑}

工具调用模块的主要功能是执行规划好的API调用路径，并将得到的结果返回给系统，从而为后续的推理和规划提供支持，最终生成符合用户需求的答案。该模块的整体逻辑可以分为两个主要部分：工具调用和工具响应解析。具体而言，我们首先根据工具的描述信息生成请求体，然后通过工具规划流程确定请求体中的参数，最后结合用户需求填充请求体中的参数值。在生成请求体后，我们通过API调用接口将请求体发送给目标API工具，以获取其响应。获得响应后，我们首先检查响应内容的长度，以决定是否需要对API响应进行压缩。最后，经过压缩的API响应将被存储在路径规划智能体的“短期记忆”中，即模型的上下文，以辅助模型生成最终结果。

\subsection{工具调用模块}

工具调用模块的主要工作就是生成API工具的请求体。在请求体的生成过程中，我们首先将与工具调用相关的内容提供给大语言模型智能体，要求其生成指定的JSON格式调用输入，包括但不限于工具调用的URL、所需输入参数及工具的登录验证信息等。接着，我们对生成的JSON数据中的参数信息进行验证，并确定请求体中各个参数的结构和格式。

首先，我们会确认生成的参数URL与工具库中的URL是否匹配。其次，参数的正确性对API工具调用的成功至关重要，因此在执行调用工具代码之前，我们会对参数进行严格的校验。在这一步中，我们验证具体的参数数量、类型和名称，并对这些部分进行精确匹配。如果缺少必要的参数或参数类型不匹配，我们将返回固定的错误信息，并要求参数解析器重新生成参数。这一过程会一直重复，直到获取正确的参数或超过最大尝试次数后放弃该API调用。

在成功验证URL和参数后，我们将调用固定的函数来执行API请求，并获取JSON格式的响应，随后将其提供给响应解析模块。

\subsubsection{工具响应解析模块}

生成请求体后，我们将其通过API调用接口发送给目标API工具，以获取相应的响应数据。响应数据通常是以JSON格式返回的，这其中可能包含大量的信息，而这些信息并不总是直接相关。在我们的实际实验中，我们通过分析发现许多API返回内容包含大量冗余信息，导致其长度过长，无法将调用结果输入到大语言模型中，直接使用大语言模型从中提取重要信息较为困难。因此，我们对API的响应结果进行了压缩。该模块的目标是在尽可能多地保留关键信息的同时减少API响应的长度，便于放入大语言模型的上下文中。

由于每个API的响应格式不是固定的，无法确定每个字段应该舍弃还是保留，因此我们采用大语言模型来分析示例响应，仅保留与用户需求相关的字段，以减小响应长度。具体步骤包括：

\begin{enumerate}
    \item \textbf{工具文档信息}：所有API工具均来自ToolBench等开源数据集，这些数据集同时包含每个API的详细描述信息和API响应示例。因此，我们可以将工具名称、工具描述、API名称、API描述、参数及API响应示例的内容以文本形式提供给大型语言模型。这部分构成了压缩模块的基本提示词。
    
    \item \textbf{详细规则指令}：我们要求大型语言模型仔细阅读API的功能描述，并保留与功能描述最相关的信息，诸如API版本、调用时间或无效信息等可以被舍弃。
    
    \item \textbf{上下文学习示例}：我们使用了三个上下文学习示例，每个示例由一个原始API响应和对应的专家撰写的压缩响应组成。我们要求压缩器以自然语言文本格式输出所有需要保留的字段，然后通过正则表达式匹配得到一个保留字段的列表，并用固定的代码逻辑筛选出相应字段的返回内容。
\end{enumerate}

在推理过程中，当API响应长度超过1024个字符时，我们会通过移除不重要的信息来压缩响应。如果压缩后的响应仍然超过1024个字符，则只保留压缩后的前1024个字符。这种方法能够有效地减少API响应长度，对API响应进行去噪。同时，也能够缓解大型语言模型有限的上下文窗口的问题，确保系统的正常调用。

通过分析我们在ToolBench中的API响应示例，其中一个API的平均响应字符串长度为xxx个字符。经过响应压缩后，最长的响应也不超过1024个字符。通过这种方式，我们有效降低了xx\%的超出上下文的情况，平均每个API响应结果节约了xx个字符。

（todo，实验部分待补充）

\section{实验与评估}

\subsection{测试数据集}
\label{subsec:test_dataset}

ToolBench。ToolBench\cite{Qin2023}是一个公开的针对工具调用的数据集，其中包含了来自49个类别的16464个真实世界的API工具的推理轨迹数据。该数据集包括三个部分，三个子数据集的难度逐级上升：G1数据集，其中目标任务所需的API都在同一个工具组；G2数据集，其中目标任务所需的API在同一个类别但是属于不同的工具组；G3数据集，其中目标任务所需的API会跨越不同类别。为了测试各个难度等级上的能力，本工作从三个类别分别抽取了350，350和300条数据构建测试集。测试集一共涉及18个category的358个工具。

API-Bank。API-Bank\cite{Li2023}是另一个公开的工具调用数据集，作者们针对模型工具调用的检索、规划能力的评估精心构建了测试数据，其中包含73个API工具，并对314个工具调用进行了标注。本工作从中抽取了300条数据作为测试集。

考虑到RapidAPIHub上的API的质量参差不齐，比如有一些API工具为废弃的，并且存在一大部分API工具为付费工具，这都可能会给测试过程引入不必要的噪声。

因此本工作首先筛选得到了一组覆盖各种类别的已知可用的高质量API工具，然后针对这些API工具，沿用ToolBench的方法构建了三个不同难度的测试数据集，作为该方法的测试数据。下面将会详细介绍数据集的构建过程。


\subsubsection{高质量API工具集筛选}

首先，我们需要定义什么是高质量的API工具。在我们的使用场景中，工具的可用性是首要考虑因素，因此必须确保筛选后的API工具都是可用的。此外，在工具选择模块中，我们使用API工具的名称和描述信息作为输入，供模型参考和选择。因此，API名称的易读性和描述的丰富性也是筛选时的重要参考标准。

同时，在保证API质量的基础上，我们也希望尽可能覆盖更多的工具类别和工具集。因此，我们从每个不同类别的工具中进行采样，选择了共计xx个类别、xx个工具集的xx个工具，作为筛选前的工具池。

基于上述规则，我们构建了一个工具筛选流程，并针对不同维度设置了相应的筛选机制。对于API工具的可用性，我们通过调用示例代码来测试每个工具的有效性。根据API的返回状态码、请求响应时间和响应内容，我们选择最合适的API。在我们评估的xx个API工具中，有xx个API的响应状态码为错误码，且有xx个API未能在规定的时间内返回。经过可用性筛选后，我们从xx个工具中筛选得到了xx个可用工具。

对于API描述的丰富性和完整性，我们采用大语言模型标注的方法进行筛选。我们构建了包含详细指令和筛选标准的提示词，并提供了few-shot样例，供模型对每个API进行评估。为加快筛选速度并节约模型调用的字数，每次将5个API进行批量判断。模型将输出一个JSON格式的列表，包含对每个API的保留或丢弃的判断。

经过第二轮筛选后，最终剩下的高质量API工具共有xx个。

画表格，介绍每个不同部分有哪些API种类。

\subsubsection{工具调用数据集构造}

为了覆盖不同难度和复杂度的用户需求，我们参考ToolBench中的分类方法，选择了三个不同难度的任务类别：单工具任务、多工具集任务和多类别任务。

\begin{enumerate}
  \item \textbf{单工具任务} \\
    该任务仅涉及一个工具，用户需求仅包含一个工具的调用。这是工具调用中最简单的情况，通常用于测试大语言模型在处理基本指令时的能力。在数据生成过程中，我们直接随机采样一些API，并引导大语言模型生成与这些API相关的用户需求。这种方法不仅能够快速生成数据，还能确保指令的有效性和准确性，适用于初学者或对工具调用不太熟悉的用户。

  \item \textbf{多工具集任务} \\
    该任务涉及多个工具集，用户需求需要调用多个工具集中的多个工具。这种任务要求大语言模型具备更高的灵活性和综合能力，能够理解不同工具之间的功能关系。在实现时，我们随机采样来自不同工具集的工具，并将其提供给大语言模型，让其生成用户需求。为了确保生成的需求合理，我们特别考虑了工具组合的有效性。对于那些功能上明显重复或无法自然组合在一起的工具API，大语言模型将直接放弃生成不合逻辑的用户需求，并重新采样一组更合理的API。这种方法有效地增强了模型在实际应用中的适应性，帮助生成更符合真实场景的用户需求。

  \item \textbf{多类别任务} \\
    该任务涉及多个类别，用户需求需要调用多个类别的多个工具。这是对大语言模型综合能力的进一步挑战，因为不同类别的工具可能具有不同的功能和用途。在实现过程中，我们同样随机采样来自不同类别的工具，并将其提供给大语言模型，促使其生成多样化的用户需求。这种多类别的设计不仅提高了数据的复杂性，还增强了模型在处理多元化需求时的能力，使其更接近于真实世界的使用场景。

\end{enumerate}

通过上述的方法，我们构建了一个共1000条数据的测试集，其中单工具、多工具集和多类别任务分别占350、350和300条。这种结构化的测试集设计使得我们能够全面评估大语言模型在处理不同复杂度的用户需求时的表现，进而优化模型的生成能力和适应性。经过人工的评估，这种方法具有较高的多样性，能够覆盖到大部分的实际场景。

\subsection{评估指标}
由于工具的多样性，对于同一个用户需求可以有多种工具调用路径。因此，我们无法事先对每个测试的输入标注单一的解决路径标准答案。由于人工评价较为费时费力，本文基于\cite{Tang2023}中的评估器构建了类似的评估体系，包含以下两个指标。我们的评估器使用的是目前能力最强的模型之一GPT-4，温度系数设置为0。（todo）

\begin{itemize}
    \item \textbf{通过率（Pass Rate）} \\
    通过率是计算在有限的工具执行步骤内完成了需求的比例。该指标衡量了系统工具调用最基本的执行能力。通过率的公式如下：

    \begin{equation}
        PR = \frac{ \#(\text{Solved}) }{ \#(\text{Solved}) + \#(\text{Unsolved}) }.
    \end{equation}

    \item \textbf{胜率（Win Rate）} \\
    胜率是评价两条针对同一需求生成的路径的偏好。在模型判断胜率的评估器的提示词中，我们预先定义了一组标准，其中包括：探索性、真实性、工具个数。胜率的公式如下：

    \begin{equation}
        WR = \frac{ \#(\text{Won}) }{ \#(\text{Won}) + \#(\text{Lost}) + \#(\text{Tie}) }.
    \end{equation}

\end{itemize}

同时，为了验证评估器与人类标注者的标注一致性，我们人工标注了100条通过率和100条胜率的数据。经过这200条数据，我们发现标注器在通过率上与人工标注的一致性达到了xxx（todo），在胜率上该数字达到了xxx（todo），这表明该基于大语言模型的标注器与人工标注的标准基本吻合。

\subsection{基准线}

为了对比本研究提出的基于Agent与知识图谱的任务编排与执行方法的效果，本文选用下列方法作为实验的基准方法。

\begin{itemize}
  \item  \textbf{基本提示方法}。基本提示方法即在大语言模型中直接输入所有候选API的信息，然后要求大语言模型输出需要调用的API的名称和参数等。
  \item  \textbf{思维链方法}\cite{Wang2023}。思维链方式在提示词中加入了"Let's think step by step"的提示信息，引导大语言模型能够进行按步骤的推理。
  \item  \textbf{ReACT方法}\cite{Yao2023}。ReACT方法通过让大语言模型不断生成Thought和Action，然后将外部的环境反馈也纳入大语言模型的上下文，让模型能够更好地进行规划。
\end{itemize}
\indent

关于大语言模型的选择，我们选择了Qwen2.5-7b和GPT-4作为基础模型，这两个模型都具有中英文双语能力，且能够对比开源模型和能力更强的闭源模型在工具能力上的区别。

\subsection{实验结果}

实验设置xxx。

结果画个表。

有哪些提升画个表。

\label{subsec:exp_results}

\subsection{错误分析}
\label{subsec:error_analysis}

对于失败的案例，分析到底是为什么会失败

\section{本章小结}
\label{sec:summary_chap4}