\chapter{实验分析}

\section{API召回向量模型实验与评估}

\subsection{模型训练及超参数设置}

根据第四章介绍的方法，我们采用了简单负样本构造和困难负样本构造两种方式，分别生成了 xxx 条数据，用于微调模型。

在模型选择方面，我们选择了 BGE 系列模型，并最终确定 bge-m3 作为基座模型。BGE 模型在训练中使用了对比学习中的常用损失函数 \textbf{InfoNCE}，即信息噪声对比估计（Information Noise-Contrastive Estimation）损失。InfoNCE 通过最大化正样本与负样本之间的互信息来优化无监督表示学习，旨在增强模型的特征判别力。

InfoNCE 的核心思想在于最大化正样本对的相似性，同时最小化负样本对的相似性，使模型学到区分能力更强的表示。给定一个查询样本 \( x \) 和其对应的正样本 \( x^+ \)，InfoNCE 损失通过引入多个负样本 \( x_i^- \) 来实现对比学习，其定义为：

\[
\mathcal{L}_{\text{InfoNCE}} = - \mathbb{E} \left[ \log \frac{\exp\left(\frac{\text{sim}(f(x), f(x^+))}{\tau}\right)}{\exp\left(\frac{\text{sim}(f(x), f(x^+))}{\tau}\right) + \sum_{i=1}^{N} \exp\left(\frac{\text{sim}(f(x), f(x_i^-))}{\tau}\right)} \right]
\]

其中：
\begin{itemize}
    \item \( f(x) \) 表示样本 \( x \) 的嵌入表示，由编码器网络生成。
    \item \( \text{sim}(a, b) \) 表示相似度函数，常用余弦相似度或点积，定义为：
    \[
    \text{sim}(a, b) = \frac{a \cdot b}{\|a\| \|b\|}
    \]
    \item \( x^+ \) 是与 \( x \) 相关的正样本。
    \item \( x_i^- \) 是与 \( x \) 无关的负样本集合，其中 \( i = 1, 2, \dots, N \)。
    \item \( \tau \) 是温度参数，用于控制对比强度，调整相似性分布。
\end{itemize}

通过最大化正样本对的相似性 \( \text{sim}(f(x), f(x^+)) \) 并最小化负样本对的相似性 \( \text{sim}(f(x), f(x_i^-)) \)，InfoNCE 损失函数能帮助模型学习更具判别力的特征。这种方法广泛应用于无监督学习，使模型在无标签数据条件下获得有效嵌入表示。

在向量模型微调前，选择合适的基座模型至关重要，因为它直接影响微调后模型的性能和效率。我们对 bge-small、bge-m3、m3e、bce 等常用向量模型进行了实验，通过性能对比最终选择 bge-m3 作为微调的基础模型。

在微调 bge-m3 过程中，我们将训练参数设置为：epoch 数量为 1，学习率为 3e-5。模型规模为 xxx，在 xxx 芯片上训练，最终获得了 xxx 的结果。


\subsection{评估指标}

我们将会用召回率（Recall）和NDCG分数来衡量微调后的工具召回器的效果。

\textbf{Recall}

Recall（召回率）是衡量推荐系统或搜索引擎中，系统成功检索到的相关项占所有相关项的比例，在本系统中即为计算搜索到的有关工具在所有相关工具的占比。它的计算公式为：

\[
\text{Recall} = \frac{|\text{Relevant Tools} \cap \text{Retrieved Tools}|}{|\text{Relevant Tools}|}
\]

其中：
- \(\text{Relevant Tools}\) 表示所有相关的工具（即正确答案的集合）。
- \(\text{Retrieved Tools}\) 表示系统返回的所有工具结果集合。

Recall 的取值范围为 [0, 1]，值越大表明系统检索出的相关项越多，即召回率越高。

在该工具召回器的实验中，我们通过衡量召回的前K个工具中包含多少正确相关工具来计算召回率。

\textbf{NDCG (Normalized Discounted Cumulative Gain)}

NDCG 是一种常用的排名质量指标，用于衡量检索结果的排序好坏。NDCG 的思想是，相关性越高的项应排在越前面，越靠前的正确答案权重越大。

NDCG 由 DCG（Discounted Cumulative Gain）归一化得到，DCG 的计算公式为：

\[
\text{DCG}_p = \sum_{i=1}^{p} \frac{2^{rel_i} - 1}{\log_2(i + 1)}
\]

其中：
- \(rel_i\) 表示结果中第 \(i\) 个文档的相关性得分。
- \(i\) 是文档在排序中的位置。

为了将 DCG 标准化，我们需要计算理想情况下的 DCG（即相关性最高的项排在最前面），称为 IDCG（Ideal DCG）：

\[
\text{IDCG}_p = \sum_{i=1}^{|REL_p|} \frac{2^{rel_i} - 1}{\log_2(i + 1)}
\]

最后，NDCG 通过 DCG 与 IDCG 的比值计算得出：

\[
\text{NDCG}_p = \frac{\text{DCG}_p}{\text{IDCG}_p}
\]

其中 \(p\) 表示计算到结果的前 \(p\) 个文档。NDCG 的值范围为 [0, 1]，越接近 1 表示排序效果越好。

在计算工具召回器的NDCG分数时，我们将所有正确的工具统一进行相关性打分，所有噪声工具统一打分为0来计算分数。

结合这两个指标，我们可以综合评估系统在 API 召回任务中的表现。Recall 关注系统找到多少相关结果，而 NDCG 则关注结果的排序质量，尤其是优先将最相关的工具排在前面。

\subsection{实验结果}

\begin{table}[!ht]
  \centering
  \caption{API工具召回实验结果}
  \label{tab:comparison}
  \begin{tabular}{lccc>{\bfseries}c} % 'lccc' defines 1 left and 3 center aligned columns, '>{\bfseries}c' makes last column bold
    \toprule
    \textbf{Method} & \textbf{I1} & \textbf{I2} & \textbf{I3} & \textbf{Average} \\ \midrule
    BCE         & 30.1 & 40.0 & 28.6 & 32.9 \\
    BGE-Small         & 42.3 & 45.6 & 33.2 & 40.4 \\
    ToolBench's API Retriever   & 46.5 & 49.1 & 38.9 & 44.8 \\
    Ours        & \textbf{58.0} & \textbf{70.6} & \textbf{62.8} & \textbf{63.8} \\ \bottomrule
  \end{tabular}
\end{table}

\subsection{实验分析}

对于上面的实验分析。

\section{基于工具图谱与深度优先遍历的API编排与调用方法实验}

\subsection{测试集构造}

数据集介绍：ToolBench\cite{Qin2023}是一个公开的针对工具调用的数据集，其中包含了来自49个类别的16464个真实世界的API工具的推理调用数据。该数据集包括三个部分，三个子数据集的难度逐级上升：G1数据集，其中目标任务所需的API都在同一个工具组；G2数据集，其中目标任务所需的API在同一个类别但是属于不同的工具组；G3数据集，其中目标任务所需的API会跨越不同类别。为了测试各个难度等级上的能力，本工作从三个类别分别抽取了350，350和300条数据构建测试集。测试集一共涉及18个category的358个工具。

考虑到RapidAPIHub上的API的质量参差不齐，比如有一些API工具为废弃的，并且存在一大部分API工具为付费工具，这都可能会给测试过程引入不必要的噪声。

因此本工作首先筛选得到了一组覆盖各种类别的已知可用的高质量API工具，然后针对这些API工具，沿用ToolBench的方法构建了三个不同难度的测试数据集，作为该方法的测试数据。下面将会详细介绍数据集的构建过程。

\subsubsection{高质量API工具集筛选}

首先，我们需要定义什么是高质量的API工具。在我们的使用场景中，工具的可用性是首要考虑因素，因此必须确保筛选后的API工具都是可用的。此外，在工具选择模块中，我们使用API工具的名称和描述信息作为输入，供模型参考和选择。因此，API名称的易读性和描述的丰富性也是筛选时的重要参考标准。

同时，在保证API质量的基础上，我们也希望尽可能覆盖更多的工具类别和工具集。因此，我们从每个不同类别的工具中进行采样，选择了共计xx个类别、xx个工具集的xx个工具，作为筛选前的工具池。

基于上述规则，我们构建了一个工具筛选流程，并针对不同维度设置了相应的筛选机制。对于API工具的可用性，我们通过调用示例代码来测试每个工具的有效性。根据API的返回状态码、请求响应时间和响应内容，我们选择最合适的API。在我们评估的xx个API工具中，有xx个API的响应状态码为错误码，且有xx个API未能在规定的时间内返回。经过可用性筛选后，我们从xx个工具中筛选得到了xx个可用工具。

对于API描述的丰富性和完整性，我们采用大语言模型标注的方法进行筛选。我们构建了包含详细指令和筛选标准的提示词，并提供了few-shot样例，供模型对每个API进行评估。为加快筛选速度并节约模型调用的字数，每次将K个API进行批量判断。模型将输出一个JSON格式的列表，包含对每个API的保留或丢弃的判断。

经过第二轮筛选后，最终剩下的高质量API工具共有xx个。

画表格，介绍每个不同部分有哪些API种类。

\subsubsection{工具调用数据集构造}

为了覆盖不同难度和复杂度的用户需求，我们参考ToolBench中的分类方法，
选择了三个不同难度的任务类别：单工具任务、多工具集任务和多类别任务。

\begin{enumerate}
  \item \textbf{单工具任务} \\
    该任务仅涉及一个工具，用户需求仅包含一个工具的调用。这是工具调用中最简单的情况，通常用于测试大语言模型在处理基本指令时的能力。在数据生成过程中，我们直接随机采样一些API，并引导大语言模型生成与这些API相关的用户需求。这种方法不仅能够快速生成数据，还能确保指令的有效性和准确性，适用于初学者或对工具调用不太熟悉的用户。

  \item \textbf{多工具集任务} \\
    该任务涉及多个工具集，用户需求需要调用多个工具集中的多个工具。这种任务要求大语言模型具备更高的灵活性和综合能力，能够理解不同工具之间的功能关系。在实现时，我们随机采样来自不同工具集的工具，并将其提供给大语言模型，让其生成用户需求。为了确保生成的需求合理，我们特别考虑了工具组合的有效性。对于那些功能上明显重复或无法自然组合在一起的工具API，大语言模型将直接放弃生成不合逻辑的用户需求，并重新采样一组更合理的API。这种方法有效地增强了模型在实际应用中的适应性，帮助生成更符合真实场景的用户需求。

  \item \textbf{多类别任务} \\
    该任务涉及多个类别，用户需求需要调用多个类别的多个工具。这是对大语言模型综合能力的进一步挑战，因为不同类别的工具可能具有不同的功能和用途。在实现过程中，我们同样随机采样来自不同类别的工具，并将其提供给大语言模型，促使其生成多样化的用户需求。这种多类别的设计不仅提高了数据的复杂性，还增强了模型在处理多元化需求时的能力，使其更接近于真实世界的使用场景。

\end{enumerate}

通过上述的方法，我们构建了一个共1000条数据的测试集，其中单工具、多工具集和多类别任务分别占350、350和300条。这种结构化的测试集设计使得我们能够全面评估大语言模型在处理不同复杂度的用户需求时的表现，进而优化模型的生成能力和适应性。经过人工的评估，这种方法具有较高的多样性，能够覆盖到大部分的实际场景。

\subsubsection{基于工具图的测试数据构造}

在测试数据构造过程中，我们参考了TaskBench的子图采样和反向指令生成方法：首先在工具图中采样一个子图，然后通过大型语言模型（LLM）将采样得到的子图转换为自然语言用户指令，从而构建测试数据集。

具体而言，我们从工具图中抽取子图，并保留抽取工具在原图中的连接关系，以表示工具之间的依赖性。我们将得到的工具子图分为两类：单个工具节点和工具链。

单个工具节点代表独立的工具调用，适用于单个工具即可完成的简单任务。工具链表示按顺序调用工具，需要多个工具依次执行，以完成较为复杂的任务。

通过上述两种子图抽样方式，我们可以模拟现实中的工具调用模式，以满足不同用户指令的需求。

我们将工具子图表示为 \( G_s = \{T_s, E_s\} \)，其中 \( T_s = \{t_{s1}, t_{s2}, \dots, t_{sk}\} \) 并且 \( k < n \)，\( E_s = \{(t_{sa}, t_{sb})\} \)，其中 \( t_{sa} \) 和 \( t_{sb} \) 属于 \( T_s \)。工具子图的抽样过程描述如下：

\[
\text{Sample}(G, \text{type}, \text{n}) \rightarrow G_s
\]

其中，\(\text{type}\) 指定抽样模式（如单节点、工具链），\(\text{n}\) 表示工具数量（范围设定为 \(\{1, 2, \dots, 5\}\)）。这些因素决定了用户指令中工具子图的拓扑结构特性和节点数规模。

接下来，基于采样得到的子图 \( G_s \)，我们使用GPT-3.5等大型语言模型（LLM）生成用户指令。此过程称为反向指令生成（BACK-INSTRUCT），用于将采样得到的工具子图转换为自然语言用户指令。具体而言，给定一个抽样得到的子图 \( G_s \)，我们定义反向指令生成过程如下，以使 LLMs 能够生成相应的指令：

\[
\text{BackInstruct1}(G_s = (T_s, E_s)) \rightarrow \text{Instruction}.
\]

在此过程中，采样得到的子图 \( G_s \) 用于指导LLM生成涵盖相关子任务及其依赖关系的用户请求。该策略保证了生成数据的复杂性和质量。

按照上述数据构造策略，我们共生成了1000条数据作为测试集，其中包括300条单工具任务和700条多工具任务。该测试集共覆盖846个工具，多工具任务的平均工具节点数为3.4，表明此数据集在工具调用任务上具有一定的复杂性。

\subsection{评估指标}
由于工具的多样性，对于同一个用户需求可以有多种工具调用路径。因此，我们无法事先对每个测试的输入标注单一的解决路径标准答案。由于人工评价较为费时费力，本文基于\cite{Tang2023}中的评估器构建了类似的评估体系，包含以下两个指标。我们的评估器使用的是目前能力最强的模型之一GPT-4，温度系数设置为0。（todo）

\begin{itemize}
    \item \textbf{通过率（Pass Rate）} \\
    通过率是计算在有限的工具执行步骤内完成了需求的比例。该指标衡量了系统工具调用最基本的执行能力。通过率的公式如下：

    \begin{equation}
        PR = \frac{ \#(\text{Solved}) }{ \#(\text{Solved}) + \#(\text{Unsolved}) }.
    \end{equation}

    \item \textbf{胜率（Win Rate）} \\
    胜率是评价两条针对同一需求生成的路径的偏好。在模型判断胜率的评估器的提示词中，我们预先定义了一组标准，其中包括：探索性、真实性、工具个数。胜率的公式如下：

    \begin{equation}
        WR = \frac{ \#(\text{Won}) }{ \#(\text{Won}) + \#(\text{Lost}) + \#(\text{Tie}) }.
    \end{equation}

\end{itemize}

同时，为了验证评估器与人类标注者的标注一致性，我们人工标注了100条通过率和100条胜率的数据。经过这200条数据，我们发现标注器在通过率上与人工标注的一致性达到了xxx（todo），在胜率上该数字达到了xxx（todo），这表明该基于大语言模型的标注器与人工标注的标准基本吻合。

\subsection{基准线}

为了对比本研究提出的基于Agent与知识图谱的任务编排与执行方法的效果，本文选用下列方法作为实验的基准方法。

\begin{itemize}
  \item  \textbf{基本提示方法}。基本提示方法即在大语言模型中直接输入所有候选API的信息，然后要求大语言模型输出需要调用的API的名称和参数等。
  \item  \textbf{思维链方法}\cite{Wang2023}。思维链方式在提示词中加入了"Let's think step by step"的提示信息，引导大语言模型能够进行按步骤的推理。
  \item  \textbf{ReACT方法}\cite{Yao2023}。ReACT方法通过让大语言模型不断生成Thought和Action，然后将外部的环境反馈也纳入大语言模型的上下文，让模型能够更好地进行规划。
\end{itemize}
\indent

关于大语言模型的选择，我们选择了Qwen2.5-7b和GPT-4作为基础模型，这两个模型都具有中英文双语能力，且能够对比开源模型和能力更强的闭源模型在工具能力上的区别。

\subsection{实验结果}

实验设置xxx。

结果画个表。

有哪些提升画个表。

\label{subsec:exp_results}

\subsection{错误分析}
\label{subsec:error_analysis}

对于失败的案例，分析到底是为什么会失败，分析下一步的改进方向。

\section{工具图谱的消融实验}



\section{本章小结}

本章主要介绍了。