@book{Yu2001,
  author    = {余敏},
  title     = {出版集团研究},
  publisher = {中国书籍出版社},
  address   = {北京},
  year      = {2001},
  pages     = {179--193}
}

@inbook{Cheng1999,
  author    = {程根伟},
  title     = {1998年长江洪水的成因与减灾对策},
  booktitle = {长江流域洪涝灾害与科技对策},
  editor    = {许厚泽 and 赵其国},
  publisher = {科学出版社},
  address   = {北京},
  year      = {1999},
  pages     = {32--36}
}

@periodical{LSC1957,
  author    = {中国图书馆学会},
  title     = {图书馆学通讯},
  publisher = {北京图书馆},
  address   = {北京},
  year      = {1957/1990},
  number    = {1-4}
}

@article{Li1999,
  author  = {李晓东 and 张庆红 and 叶瑾琳},
  title   = {气候学研究的若干理论问题},
  journal = {北京大学学报: 自然科学版},
  year    = {1999},
  volume  = {35},
  number  = {1},
  pages   = {101--106}
}

@patent{Jiang1989,
  author = {姜锡洲},
  title  = {一种温热外敷药制备方案},
  number = {中国, 88105607.3},
  date   = {1989-07-26}
}

@inproceedings{Li2018,
   author = {Hongwei Li and Sirui Li and Jiamou Sun and Zhenchang Xing and Xin Peng and Mingwei Liu and Xuejiao Zhao},
   booktitle = {2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
   pages = {183-193},
   title = {Improving api caveats accessibility by mining api caveats knowledge graph},
   year = {2018},
}

@article{Jiang2023,
   abstract = {Knowledge graphs (KGs) are structured representations of diversified knowledge. They are widely used in various intelligent applications. In this article, we provide a comprehensive survey on the evolution of various types of knowledge graphs (i.e., static KGs, dynamic KGs, temporal KGs, and event KGs) and techniques for knowledge extraction and reasoning. Furthermore, we introduce the practical applications of different types of KGs, including a case study in financial analysis. Finally, we propose our perspective on the future directions of knowledge engineering, including the potential of combining the power of knowledge graphs and large language models (LLMs), and the evolution of knowledge extraction, reasoning, and representation.},
   author = {Xuhui Jiang and Chengjin Xu and Yinghan Shen and Xun Sun and Lumingyuan Tang and Saizhuo Wang and Zhongwu Chen and Yuanzhuo Wang and Jian Guo},
   month = {10},
   title = {On the Evolution of Knowledge Graphs: A Survey and Perspective},
   year = {2023},
}

@online{Hopkinson1999,
  author  = {Hopkinson, A.},
  title   = {{UNIMARC} and metadata: Dublin core},
  url     = {http://www.ifla.org/IV/ifla64/138-161e.htm},
  urldate = {1999-12-08}
}

@book{Jiang1998,
  author    = {蒋有绪 and 郭泉水 and 马娟 and others},
  title     = {中国森林群落分类及其群落学特征},
  publisher = {科学出版社},
  address   = {北京},
  year      = {1998},
  pages     = {11--12}
}

@proceedings{CSTAM1990,
  editor    = {中国力学学会},
  title     = {第 3 届全国实验流体力学学术会议论文集},
  publisher = {天津大学出版社},
  address   = {天津},
  year      = {1990},
  pages     = {20--24}
}

@report{WHO1970,
  author      = {{World Health Organization}},
  title       = {Factors regulating the immune response: report of {WHO} Scientific Group},
  institution = {WHO},
  location    = {Geneva},
  year        = {1970}
}

@phdthesis{Zhang1998,
  author      = {张志祥},
  title       = {间断动力系统的随机扰动及其在守恒律方程中的应用},
  institution = {北京大学数学学院},
  address     = {北京},
  year        = {1998},
  pages       = {50--55}
}

@patent{HBLZ2001,
  author  = {河北绿洲生态环境科技有限公司},
  title   = {一种荒漠化地区生态植被综合培育种植方法},
  date    = {2001-10-24},
  number  = {中国, 01129210.5},
  url     = {http://211.152.9.47/sipoasp/zlijs/hyjs-yx-new.asp?recid=01129210.5&leixin},
  urldate = {2002-05-28}
}

@standard{GBT2659,
  author     = {国家标准局信息分类编码研究所},
  title      = {GB/T 2659-1986 世界各国和地区名称代码},
  bookauthor = {全国文献工作标准化技术委员会},
  booktitle  = {文献工作国家标准汇编: 3},
  publisher  = {中国标准出版社},
  address    = {北京},
  year       = {1988},
  pages      = {59--92}
}

@article{Li2000,
  author  = {李炳穆},
  title   = {理想的图书管理员和信息专家的素养与形象},
  journal = {图书情报工作},
  year    = {2000},
  number  = {2},
  pages   = {5--8}
}

@article{Ding2000,
  author  = {丁文祥},
  title   = {数字革命与竞争国际化},
  journal = {中国青年报},
  date    = {2000-11-20},
  number  = {15},
  note    = {news}
}

@article{Jiang1999,
  author  = {江向东},
  title   = {互联网环境下的信息处理与图书管理系统解决方案},
  journal = {情报学报},
  year    = {1999},
  volume  = {18},
  number  = {2},
  pages   = {4},
  url     = {http://www.chinainfo.gov.cn/periodical/qbxb/qbxb99/qbxb990203},
  urldate = {2000-01-18}
}


@article{Christine1998,
  author  = {Christine, Mlot},
  title   = {Plant physiology: plant biology in the Genome Era},
  journal = {Science},
  year    = {1998},
  volume  = {281},
  pages   = {331--332},
  url     = {http://www.sciencemag.org/cgi/collection/anatmorp},
  urldate = {1998-09-23}
}

@online{Xiao2001,
  author  = {萧钰},
  title   = {出版业信息化迈人快车道},
  date    = {2001-12-19},
  url     = {http://www.creader.com/news/20011219/200112190019.html},
  urldate = {2002-04-15}
}

@article{Yang1999,
  author  = {杨瑞林 and 李力军 and 李玉成},
  title   = {新型低合金高强韧性耐磨钢的研究},
  journal = {钢铁},
  year    = {1999},
  volume  = {34},
  number  = {7},
  pages   = {41--45}
}

@article{Schinstock2000,
  author  = {Schinstock, D. E. and Cuttino, J. F.},
  title   = {Real time kinematic solutions of a non-contacting, three dimensional metrology frame},
  journal = {Precision Engineering},
  year    = {2000},
  volume  = {24},
  number  = {1},
  pages   = {70--76}
}

@book{Wen1990,
  author    = {温诗铸},
  title     = {摩擦学原理},
  publisher = {清华大学出版社},
  address   = {北京},
  year      = {1990},
  pages     = {296--300}
}

@standard{GBT16159,
  author    = {国家技术监督局},
  title     = {GB/T 16159-1996 汉语拼音正词法基本规则},
  publisher = {中国标准出版社},
  address   = {北京},
  year      = {1996}
}

@inproceedings{fan2024survey,
  title={A survey on rag meeting llms: Towards retrieval-augmented large language models},
  author={Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={6491--6501},
  year={2024}
}

@article{shen2023taskbench,
  title={Taskbench: Benchmarking large language models for task automation},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Zhang, Wenqi and Ren, Kan and Yuan, Siyu and Lu, Weiming and Li, Dongsheng and Zhuang, Yueting},
  journal={arXiv preprint arXiv:2311.18760},
  year={2023}
}

@inproceedings{ma2025m,
  title={m \&m’s: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks},
  author={Ma, Zixian and Huang, Weikai and Zhang, Jieyu and Gupta, Tanmay and Krishna, Ranjay},
  booktitle={European Conference on Computer Vision},
  pages={18--34},
  year={2025},
  organization={Springer}
}

@article{Shen2023,
   abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.},
   author = {Yongliang Shen and Kaitao Song and Xu Tan and Dongsheng Li and Weiming Lu and Yueting Zhuang and Zhejiang University and Microsoft Research Asia},
   month = {3},
   title = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face},
   url = {https://arxiv.org/abs/2303.17580v4},
   year = {2023},
}

@article{Tang2023,
   abstract = {Enabling large language models to utilize real-world tools effectively is crucial for achieving embodied intelligence. Existing approaches to tool learning have either primarily relied on extremely large language models, such as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or utilized supervised learning to train limited scopes of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a diverse tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first automatically creates a highly diversified tool-use corpus by building a multi-agent simulation environment. The corpus contains 3938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subsequently, the constructed corpus is employed to fine-tune compact language models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the ability of these models to utilize previously unseen tools without specific training. Experimental results demonstrate that ToolAlpaca achieves effective generalized tool-use capabilities comparable to those of extremely large language models like GPT-3.5, demonstrating that learning generalized tool-use ability is feasible for compact language models.},
   author = {Qiaoyu Tang and Ziliang Deng and Hongyu Lin and Xianpei Han and Qiao Liang and Boxi Cao and Le Sun},
   month = {6},
   title = {ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases},
   url = {https://arxiv.org/abs/2306.05301v2},
   year = {2023},
}

@article{Mekala2024,
   author = {Dheeraj Mekala and Jason Weston and Jack Lanchantin and Roberta Raileanu and Maria Lomeli and Jingbo Shang and Jane Dwivedi-Yu},
   journal = {arXiv preprint arXiv:2402.14158},
   title = {TOOLVERIFIER: Generalization to New Tools via Self-Verification},
   year = {2024},
}

@misc{Luo2023,
   abstract = {Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.},
   author = {Linhao Luo and Yuan-Fang Li and Gholamreza Haffari and Shirui Pan},
   doi = {10.48550/arXiv.2310.01061},
   keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
   month = {10},
   note = {Comment: 22 pages, 4 figures
arXiv:2310.01061 [cs]},
   publisher = {arXiv},
   title = {Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning},
   url = {http://arxiv.org/abs/2310.01061},
   year = {2023},
}

@misc{Wang2023a,
   abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
   author = {Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
   doi = {10.48550/arXiv.2203.11171},
   keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
   month = {3},
   note = {Comment: Published at ICLR 2023. V2: added PaLM results; V3: added UL2 results; V4: camera ready version at ICLR 2023
arXiv:2203.11171 [cs]},
   publisher = {arXiv},
   title = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
   url = {http://arxiv.org/abs/2203.11171},
   year = {2023},
}

@misc{Weng2023,
   author = {Lilian Weng},
   month = {6},
   title = {LLM Powered Autonomous Agents},
   url = {https://lilianweng.github.io/posts/2023-06-23-agent/},
   year = {2023},
}

@article{Ge2024,
   author = {Yingqiang Ge and Wenyue Hua and Kai Mei and Juntao Tan and Shuyuan Xu and Zelong Li and Yongfeng Zhang and others},
   journal = {Advances in Neural Information Processing Systems},
   title = {Openagi: When llm meets domain experts},
   volume = {36},
   year = {2024},
}

@article{Wang2023c,
   abstract = {Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
   author = {Lei Wang and Chen Ma and Xueyang Feng and Zeyu Zhang and Hao Yang and Jingsen Zhang and Zhiyuan Chen and Jiakai Tang and Xu Chen and Yankai Lin and Wayne Xin Zhao and Zhewei Wei and Ji-Rong Wen},
   month = {8},
   title = {A Survey on Large Language Model based Autonomous Agents},
   year = {2023},
}

@misc{Zalta2019,
   author = {E N Zalta Schlossera M.},
   publisher = {Metaphysics Research Lab, Stanford, Winter},
   title = {The Stanford Encyclopedia of Philosophy},
   year = {2019},
}

@article{Ye2021,
   abstract = {Existing KBQA approaches, despite achieving strong performance on i.i.d. test data, often struggle in generalizing to questions involving unseen KB schema items. Prior ranking-based approaches have shown some success in generalization, but suffer from the coverage issue. We present RnG-KBQA, a Rank-and-Generate approach for KBQA, which remedies the coverage issue with a generation model while preserving a strong generalization capability. Our approach first uses a contrastive ranker to rank a set of candidate logical forms obtained by searching over the knowledge graph. It then introduces a tailored generation model conditioned on the question and the top-ranked candidates to compose the final logical form. We achieve new state-of-the-art results on GrailQA and WebQSP datasets. In particular, our method surpasses the prior state-of-the-art by a large margin on the GrailQA leaderboard. In addition, RnG-KBQA outperforms all prior approaches on the popular WebQSP benchmark, even including the ones that use the oracle entity linking. The experimental results demonstrate the effectiveness of the interplay between ranking and generation, which leads to the superior performance of our proposed approach across all settings with especially strong improvements in zero-shot generalization.},
   author = {Xi Ye and Semih Yavuz and Kazuma Hashimoto and Yingbo Zhou and Caiming Xiong},
   doi = {10.18653/v1/2022.acl-long.417},
   isbn = {9781955917216},
   issn = {0736587X},
   journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
   month = {9},
   pages = {6032-6043},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {RnG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering},
   volume = {1},
   url = {https://arxiv.org/abs/2109.08678v2},
   year = {2021},
}

@article{Shu2022,
   abstract = {Pre-trained language models (PLMs) have shown their effectiveness in multiple scenarios. However, KBQA remains challenging, especially regarding coverage and generalization settings. This is due to two main factors: i) understanding the semantics of both questions and relevant knowledge from the KB; ii) generating executable logical forms with both semantic and syntactic correctness. In this paper, we present a new KBQA model, TIARA, which addresses those issues by applying multi-grained retrieval to help the PLM focus on the most relevant KB contexts, viz., entities, exemplary logical forms, and schema items. Moreover, constrained decoding is used to control the output space and reduce generation errors. Experiments over important benchmarks demonstrate the effectiveness of our approach. TIARA outperforms previous SOTA, including those using PLMs or oracle entity annotations, by at least 4.1 and 1.1 F1 points on GrailQA and WebQuestionsSP, respectively.},
   author = {Yiheng Shu and Zhiwei Yu and Yuhan Li and Börje F. Karlsson and Tingting Ma and Yuzhong Qu and Chin Yew Lin},
   doi = {10.18653/v1/2022.emnlp-main.555},
   journal = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022},
   month = {10},
   pages = {8108-8121},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {TIARA: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Bases},
   url = {https://arxiv.org/abs/2210.12925v1},
   year = {2022},
}

@misc{AlKhamissi2022,
   abstract = {Recently, there has been a surge of interest in the NLP community on the use of pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have shown that LMs trained on a sufficiently large (web) corpus will encode a significant amount of knowledge implicitly in its parameters. The resulting LM can be probed for different kinds of knowledge and thus acting as a KB. This has a major advantage over traditional KBs in that this method requires no human supervision. In this paper, we present a set of aspects that we deem a LM should have to fully act as a KB, and review the recent literature with respect to those aspects.},
   author = {Badr AlKhamissi and Millicent Li and Asli Celikyilmaz and Mona Diab and Marjan Ghazvininejad},
   keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
   month = {4},
   note = {Comment: Preprint
arXiv:2204.06031 [cs]},
   publisher = {arXiv},
   title = {A Review on Language Models as Knowledge Bases},
   url = {http://arxiv.org/abs/2204.06031},
   year = {2022},
}

@article{Ji2023,
   author = {Ziwei Ji and Nayeon Lee and Rita Frieske and Tiezheng Yu and Dan Su and Yan Xu and Etsuko Ishii and Ye Jin Bang and Andrea Madotto and Pascale Fung},
   issue = {12},
   journal = {ACM Computing Surveys},
   pages = {1-38},
   publisher = {ACM New York, NY},
   title = {Survey of hallucination in natural language generation},
   volume = {55},
   year = {2023},
}

@misc{Wang2023b,
   abstract = {We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.},
   author = {Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar},
   doi = {10.48550/arXiv.2305.16291},
   keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
   month = {10},
   note = {Comment: Project website and open-source codebase: https://voyager.minedojo.org/
arXiv:2305.16291 [cs]},
   publisher = {arXiv},
   title = {Voyager: An Open-Ended Embodied Agent with Large Language Models},
   url = {http://arxiv.org/abs/2305.16291},
   year = {2023},
}

@article{Huang2022,
   author = {Wenlong Huang and Fei Xia and Ted Xiao and Harris Chan and Jacky Liang and Pete Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and others},
   journal = {arXiv preprint arXiv:2207.05608},
   title = {Inner monologue: Embodied reasoning through planning with language models},
   year = {2022},
}

@article{Du2024,
   author = {Yu Du and Fangyun Wei and Hongyang Zhang},
   journal = {arXiv preprint arXiv:2402.04253},
   title = {Anytool: Self-reflective, hierarchical agents for large-scale api calls},
   year = {2024},
}


@article{Zhang2019,
   abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from https://github.com/thunlp/ERNIE.},
   author = {Zhengyan Zhang and Xu Han and Zhiyuan Liu and Xin Jiang and Maosong Sun and Qun Liu},
   month = {5},
   title = {ERNIE: Enhanced Language Representation with Informative Entities},
   year = {2019},
}

@article{chen2024bge,
  title={Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation},
  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
  journal={arXiv preprint arXiv:2402.03216},
  year={2024}
}

@article{Lin2019,
   abstract = {Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.},
   author = {Bill Yuchen Lin and Xinyue Chen and Jamin Chen and Xiang Ren},
   month = {9},
   title = {KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning},
   year = {2019},
}

@article{Yasunaga2022,
   abstract = {Pretraining a language model (LM) on text has been shown to help various downstream NLP tasks. Recent works show that a knowledge graph (KG) can complement text data, offering structured background knowledge that provides a useful scaffold for reasoning. However, these works are not pretrained to learn a deep fusion of the two modalities at scale, limiting the potential to acquire fully joint representations of text and KG. Here we propose DRAGON (Deep Bidirectional Language-Knowledge Graph Pretraining), a self-supervised approach to pretraining a deeply joint language-knowledge foundation model from text and KG at scale. Specifically, our model takes pairs of text segments and relevant KG subgraphs as input and bidirectionally fuses information from both modalities. We pretrain this model by unifying two self-supervised reasoning tasks, masked language modeling and KG link prediction. DRAGON outperforms existing LM and LM+KG models on diverse downstream tasks including question answering across general and biomedical domains, with +5\% absolute gain on average. In particular, DRAGON achieves notable performance on complex reasoning about language and knowledge (+10\% on questions involving long contexts or multi-step reasoning) and low-resource QA (+8\% on OBQA and RiddleSense), and new state-of-the-art results on various BioNLP tasks. Our code and trained models are available at https://github.com/michiyasunaga/dragon.},
   author = {Michihiro Yasunaga and Antoine Bosselut and Hongyu Ren and Xikun Zhang and Christopher D Manning and Percy Liang and Jure Leskovec},
   month = {10},
   title = {Deep Bidirectional Language-Knowledge Graph Pretraining},
   year = {2022},
}

@article{Xi2023,
   abstract = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.},
   author = {Zhiheng Xi and Wenxiang Chen and Xin Guo and Wei He and Yiwen Ding and Boyang Hong and Ming Zhang and Junzhe Wang and Senjie Jin and Enyu Zhou and Rui Zheng and Xiaoran Fan and Xiao Wang and Limao Xiong and Yuhao Zhou and Weiran Wang and Changhao Jiang and Yicheng Zou and Xiangyang Liu and Zhangyue Yin and Shihan Dou and Rongxiang Weng and Wensen Cheng and Qi Zhang and Wenjuan Qin and Yongyan Zheng and Xipeng Qiu and Xuanjing Huang and Tao Gui},
   month = {9},
   title = {The Rise and Potential of Large Language Model Based Agents: A Survey},
   year = {2023},
}

@book{Sutton2018,
   author = {Richard S Sutton and Andrew G Barto},
   publisher = {MIT press},
   title = {Reinforcement learning: An introduction},
   year = {2018},
}

@article{Wooldridge1995,
   author = {Michael Wooldridge and Nicholas R Jennings},
   issue = {2},
   journal = {The knowledge engineering review},
   pages = {115-152},
   publisher = {Cambridge University Press},
   title = {Intelligent agents: Theory and practice},
   volume = {10},
   year = {1995},
}

@article{Choudhary2023,
   abstract = {Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show that the performance of our approach improves proportionally to the increase in size of the underlying LLM, enabling the integration of the latest advancements in LLMs for logical reasoning over KGs. Our work presents a new direction for addressing the challenges of complex KG reasoning and paves the way for future research in this area.},
   author = {Nurendra Choudhary and Chandan K. Reddy},
   month = {5},
   title = {Complex Logical Reasoning over Knowledge Graphs using Large Language Models},
   year = {2023},
}

@inproceedings{Raman2022,
   author = {Shreyas Sundara Raman and Vanya Cohen and Eric Rosen and Ifrah Idrees and David Paulius and Stefanie Tellex},
   booktitle = {NeurIPS 2022 Foundation Models for Decision Making Workshop},
   title = {Planning with large language models via corrective re-prompting},
   year = {2022},
}

@article{Yao2023a,
   abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.},
   author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
   month = {5},
   title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
   year = {2023},
}

@misc{Yao2023b,
   abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
   author = {Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
   doi = {10.48550/arXiv.2210.03629},
   keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
   month = {3},
   note = {Comment: v3 is the ICLR camera ready version with some typos fixed. Project site with code: https://react-lm.github.io
arXiv:2210.03629 [cs]},
   publisher = {arXiv},
   title = {ReAct: Synergizing Reasoning and Acting in Language Models},
   url = {http://arxiv.org/abs/2210.03629},
   year = {2023},
}

@article{Besta2023,
   abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by >31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
   author = {Maciej Besta and Nils Blach and Ales Kubicek and Robert Gerstenberger and Lukas Gianinazzi and Joanna Gajda and Tomasz Lehmann and Michal Podstawski and Hubert Niewiadomski and Piotr Nyczyk and Torsten Hoefler},
   month = {8},
   title = {Graph of Thoughts: Solving Elaborate Problems with Large Language Models},
   url = {https://arxiv.org/abs/2308.09687v3},
   year = {2023},
}

@misc{Ma,
   author = {马展 and 王岩 and 王微微 and 赵瑞莲},
   title = {基于多源信息融合的API知识图谱构建},
   url = {http://cnjournals.com/view_abstract.aspx?aid=CFC3B0096A6D80B2BB9723DA2B12C510&jid=D4F6864C950C88FFCE5B6C948A639E39&pcid=5B3AB970F71A803DEACDC0559115BFCF0A068CD97DD29835&yid=9475FABC7A03F4AB},
}

@article{Wang2021,
   author = {Xin Wang and Xiao Liu and Jin Liu and Xiaomei Chen and Hao Wu},
   issue = {3},
   journal = {World Wide Web},
   pages = {869-894},
   publisher = {Springer},
   title = {A novel knowledge graph embedding based API recommendation method for Mashup development},
   volume = {24},
   year = {2021},
}

@article{OpenAI2023,
   abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
   author = {OpenAI},
   month = {3},
   title = {GPT-4 Technical Report},
   url = {https://arxiv.org/abs/2303.08774v3},
   year = {2023},
}

@article{Yang2023,
   abstract = {Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.},
   author = {Aiyuan Yang and Bin Xiao and Bingning Wang and Borong Zhang and Ce Bian and Chao Yin and Chenxu Lv and Da Pan and Dian Wang and Dong Yan and Fan Yang and Fei Deng and Feng Wang and Feng Liu and Guangwei Ai and Guosheng Dong and Haizhou Zhao and Hang Xu and Haoze Sun and Hongda Zhang and Hui Liu and Jiaming Ji and Jian Xie and JunTao Dai and Kun Fang and Lei Su and Liang Song and Lifeng Liu and Liyun Ru and Luyao Ma and Mang Wang and Mickel Liu and MingAn Lin and Nuolan Nie and Peidong Guo and Ruiyang Sun and Tao Zhang and Tianpeng Li and Tianyu Li and Wei Cheng and Weipeng Chen and Xiangrong Zeng and Xiaochuan Wang and Xiaoxi Chen and Xin Men and Xin Yu and Xuehai Pan and Yanjun Shen and Yiding Wang and Yiyu Li and Youxin Jiang and Yuchen Gao and Yupeng Zhang and Zenan Zhou and Zhiying Wu},
   month = {9},
   title = {Baichuan 2: Open Large-scale Language Models},
   year = {2023},
}

@article{Zeng2023,
   abstract = {Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining AgentInstruct with open-source instructions from general domains. AgentTuning is used to instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show that AgentTuning enables LLMs' agent capabilities without compromising general abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the AgentInstruct and AgentLM-7B, 13B, and 70B models at https://github.com/THUDM/AgentTuning, serving open and powerful alternatives to commercial LLMs for agent tasks.},
   author = {Aohan Zeng and Mingdao Liu and Rui Lu and Bowen Wang and Xiao Liu and Yuxiao Dong and Jie Tang},
   month = {10},
   title = {AgentTuning: Enabling Generalized Agent Abilities for LLMs},
   url = {https://arxiv.org/abs/2310.12823v2},
   year = {2023},
}

@article{Touvron2023,
   abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
   author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
   month = {2},
   title = {LLaMA: Open and Efficient Foundation Language Models},
   url = {https://arxiv.org/abs/2302.13971v1},
   year = {2023},
}

@article{Ling2019,
   author = {Chun-Yang Ling and Yan-Zhen Zou and Ze-Qi Lin and Bing Xie},
   journal = {Journal of Computer Science and Technology},
   pages = {993-1006},
   publisher = {Springer},
   title = {Graph embedding based API graph search and recommendation},
   volume = {34},
   year = {2019},
}

@misc{Song2023,
   abstract = {Tool-augmented large language models (LLMs) have achieved remarkable progress in tackling a broad range of tasks. However, existing methods are mainly restricted to specifically designed tools and fail to fulfill complex instructions, having great limitations when confronted with real-world scenarios. In this paper, we explore a more realistic scenario by connecting LLMs with RESTful APIs, which adhere to the widely adopted REST software architectural style for web service development. To address the practical challenges of tackling complex instructions, we propose RestGPT, which exploits the power of LLMs and conducts a coarse-to-fine online planning mechanism to enhance the abilities of task decomposition and API selection. RestGPT also contains an API executor tailored for calling RESTful APIs, which can meticulously formulate parameters and parse API responses. To fully evaluate the performance of RestGPT, we propose RestBench, a high-quality benchmark which consists of two real-world scenarios and human-annotated instructions with gold solution paths. Experiments show that RestGPT is able to achieve impressive results in complex tasks and has strong robustness, which paves a new way towards AGI. RestGPT and RestBench is publicly available at https://restgpt.github.io/.},
   author = {Yifan Song and Weimin Xiong and Dawei Zhu and Wenhao Wu and Han Qian and Mingbo Song and Hailiang Huang and Cheng Li and Ke Wang and Rong Yao and Ye Tian and Sujian Li},
   doi = {10.48550/arXiv.2306.06624},
   keywords = {Computer Science - Computation and Language},
   month = {8},
   note = {Comment: Add RestBench to evaluate RestGPT
arXiv:2306.06624 [cs]},
   publisher = {arXiv},
   title = {RestGPT: Connecting Large Language Models with Real-World RESTful APIs},
   url = {http://arxiv.org/abs/2306.06624},
   year = {2023},
}

@article{Ruan2023,
   abstract = {With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.},
   author = {Jingqing Ruan and Yihong Chen and Bin Zhang and Zhiwei Xu and Tianpeng Bao and Guoqing Du and Shiwei Shi and Hangyu Mao and Ziyue Li and Xingyu Zeng and Rui Zhao},
   month = {8},
   title = {TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage},
   year = {2023},
}

@article{Miao2023,
   abstract = {The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on three datasets (GSM8K, MathQA, and MATH) and find that it successfully recognizes errors and, in turn, increases final answer accuracies.},
   author = {Ning Miao and Yee Whye Teh and Tom Rainforth},
   month = {8},
   title = {SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning},
   year = {2023},
}

@article{Liu2024,
   author = {Xukun Liu and Zhiyuan Peng and Xiaoyuan Yi and Xing Xie and Lirong Xiang and Yuchen Liu and Dongkuan Xu},
   journal = {arXiv preprint arXiv:2403.00839},
   title = {ToolNet: Connecting large language models with massive tools via tool graph},
   year = {2024},
}

@article{powers2020evaluation,
  title={Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation},
  author={Powers, David MW},
  journal={arXiv preprint arXiv:2010.16061},
  year={2020}
}

@inproceedings{wang2013theoretical,
  title={A theoretical analysis of NDCG type ranking measures},
  author={Wang, Yining and Wang, Liwei and Li, Yuanzhi and He, Di and Liu, Tie-Yan},
  booktitle={Conference on learning theory},
  pages={25--54},
  year={2013},
  organization={PMLR}
}

@article{buckland1994relationship,
  title={The relationship between recall and precision},
  author={Buckland, Michael and Gey, Fredric},
  journal={Journal of the American society for information science},
  volume={45},
  number={1},
  pages={12--19},
  year={1994},
  publisher={Wiley Online Library}
}

@article{Sun2023,
   author = {Jiashuo Sun and Chengjin Xu and Lumingyuan Tang and Saizhuo Wang and Chen Lin and Yeyun Gong and Heung-Yeung Shum and Jian Guo},
   journal = {arXiv preprint arXiv:2307.07697},
   title = {Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph},
   year = {2023},
}

@article{Ma2024,
   author = {Shengjie Ma and Chengjin Xu and Xuhui Jiang and Muzhi Li and Huaren Qu and Jian Guo},
   journal = {arXiv preprint arXiv:2407.10805},
   title = {Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval},
   year = {2024},
}

@article{Liu2023a,
   author = {Zhaoyang Liu and Zeqiang Lai and Zhangwei Gao and Erfei Cui and Zhiheng Li and Xizhou Zhu and Lewei Lu and Qifeng Chen and Yu Qiao and Jifeng Dai and others},
   journal = {arXiv preprint arXiv:2310.17796},
   title = {Controlllm: Augment language models with tools by searching on graphs},
   year = {2023},
}

@article{Liu2023b,
   abstract = {Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality multi-turn alignment data could improve agent performance. Datasets, environments, and an integrated evaluation package for AgentBench are released at \url\{https://github.com/THUDM/AgentBench\}.},
   author = {Xiao Liu and Hao Yu and Hanchen Zhang and Yifan Xu and Xuanyu Lei and Hanyu Lai and Yu Gu and Hangliang Ding and Kaiwen Men and Kejuan Yang and Shudan Zhang and Xiang Deng and Aohan Zeng and Zhengxiao Du and Chenhui Zhang and Sheng Shen and Tianjun Zhang and Yu Su and Huan Sun and Minlie Huang and Yuxiao Dong and Jie Tang},
   month = {8},
   title = {AgentBench: Evaluating LLMs as Agents},
   year = {2023},
}

@article{Jones1972,
   author = {Karen Sparck Jones},
   issue = {1},
   journal = {Journal of documentation},
   pages = {11-21},
   publisher = {MCB UP Ltd},
   title = {A statistical interpretation of term specificity and its application in retrieval},
   volume = {28},
   year = {1972},
}

@misc{moka2024m3e,
  title = {{M3E-base}: Multilingual Embedding Model},
  author = {{Moka AI}},
  year = {2024},
  url = {https://huggingface.co/moka-ai/m3e-base},
  note = {Accessed: 2024-11-15}
}

@misc{netease2024BCEmbedding,
  title = {{BCEmbedding}: Bilingual Contextual Embedding for Cross-lingual Text Understanding},
  author = {{Netease Youdao Research Team}},
  year = {2024},
  url = {https://github.com/netease-youdao/BCEmbedding},
  note = {Accessed: 2024-11-15}
}

@article{Qin2023,
   abstract = {Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.},
   author = {Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Lauren Hong and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
   month = {7},
   title = {ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs},
   year = {2023},
}

@article{Robertson2009,
   author = {Stephen Robertson and Hugo Zaragoza and others},
   issue = {4},
   journal = {Foundations and Trends® in Information Retrieval},
   pages = {333-389},
   publisher = {Now Publishers, Inc.},
   title = {The probabilistic relevance framework: BM25 and beyond},
   volume = {3},
   year = {2009},
}

@misc{Li2023c,
   abstract = {Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question.},
   author = {Minghao Li and Yingxiu Zhao and Bowen Yu and Feifan Song and Hangyu Li and Haiyang Yu and Zhoujun Li and Fei Huang and Yongbin Li},
   doi = {10.48550/arXiv.2304.08244},
   keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
   month = {10},
   note = {Comment: EMNLP 2023
arXiv:2304.08244 [cs]},
   publisher = {arXiv},
   title = {API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs},
   url = {http://arxiv.org/abs/2304.08244},
   year = {2023},
}


@Comment{jabref-meta: databaseType:biblatex;}
