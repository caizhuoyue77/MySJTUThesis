@book{Yu2001,
  author    = {余敏},
  title     = {出版集团研究},
  publisher = {中国书籍出版社},
  address   = {北京},
  year      = {2001},
  pages     = {179--193}
}

@inbook{Cheng1999,
  author    = {程根伟},
  title     = {1998年长江洪水的成因与减灾对策},
  booktitle = {长江流域洪涝灾害与科技对策},
  editor    = {许厚泽 and 赵其国},
  publisher = {科学出版社},
  address   = {北京},
  year      = {1999},
  pages     = {32--36}
}

@periodical{LSC1957,
  author    = {中国图书馆学会},
  title     = {图书馆学通讯},
  publisher = {北京图书馆},
  address   = {北京},
  year      = {1957/1990},
  number    = {1-4}
}

@article{Li1999,
  author  = {李晓东 and 张庆红 and 叶瑾琳},
  title   = {气候学研究的若干理论问题},
  journal = {北京大学学报: 自然科学版},
  year    = {1999},
  volume  = {35},
  number  = {1},
  pages   = {101--106}
}

@patent{Jiang1989,
  author = {姜锡洲},
  title  = {一种温热外敷药制备方案},
  number = {中国, 88105607.3},
  date   = {1989-07-26}
}

@online{Hopkinson1999,
  author  = {Hopkinson, A.},
  title   = {{UNIMARC} and metadata: Dublin core},
  url     = {http://www.ifla.org/IV/ifla64/138-161e.htm},
  urldate = {1999-12-08}
}

@book{Jiang1998,
  author    = {蒋有绪 and 郭泉水 and 马娟 and others},
  title     = {中国森林群落分类及其群落学特征},
  publisher = {科学出版社},
  address   = {北京},
  year      = {1998},
  pages     = {11--12}
}

@proceedings{CSTAM1990,
  editor    = {中国力学学会},
  title     = {第 3 届全国实验流体力学学术会议论文集},
  publisher = {天津大学出版社},
  address   = {天津},
  year      = {1990},
  pages     = {20--24}
}

@report{WHO1970,
  author      = {{World Health Organization}},
  title       = {Factors regulating the immune response: report of {WHO} Scientific Group},
  institution = {WHO},
  location    = {Geneva},
  year        = {1970}
}

@phdthesis{Zhang1998,
  author      = {张志祥},
  title       = {间断动力系统的随机扰动及其在守恒律方程中的应用},
  institution = {北京大学数学学院},
  address     = {北京},
  year        = {1998},
  pages       = {50--55}
}

@patent{HBLZ2001,
  author  = {河北绿洲生态环境科技有限公司},
  title   = {一种荒漠化地区生态植被综合培育种植方法},
  date    = {2001-10-24},
  number  = {中国, 01129210.5},
  url     = {http://211.152.9.47/sipoasp/zlijs/hyjs-yx-new.asp?recid=01129210.5&leixin},
  urldate = {2002-05-28}
}

@standard{GBT2659,
  author     = {国家标准局信息分类编码研究所},
  title      = {GB/T 2659-1986 世界各国和地区名称代码},
  bookauthor = {全国文献工作标准化技术委员会},
  booktitle  = {文献工作国家标准汇编: 3},
  publisher  = {中国标准出版社},
  address    = {北京},
  year       = {1988},
  pages      = {59--92}
}

@article{Li2000,
  author  = {李炳穆},
  title   = {理想的图书管理员和信息专家的素养与形象},
  journal = {图书情报工作},
  year    = {2000},
  number  = {2},
  pages   = {5--8}
}

@article{Ding2000,
  author  = {丁文祥},
  title   = {数字革命与竞争国际化},
  journal = {中国青年报},
  date    = {2000-11-20},
  number  = {15},
  note    = {news}
}

@article{Jiang1999,
  author  = {江向东},
  title   = {互联网环境下的信息处理与图书管理系统解决方案},
  journal = {情报学报},
  year    = {1999},
  volume  = {18},
  number  = {2},
  pages   = {4},
  url     = {http://www.chinainfo.gov.cn/periodical/qbxb/qbxb99/qbxb990203},
  urldate = {2000-01-18}
}


@article{Christine1998,
  author  = {Christine, Mlot},
  title   = {Plant physiology: plant biology in the Genome Era},
  journal = {Science},
  year    = {1998},
  volume  = {281},
  pages   = {331--332},
  url     = {http://www.sciencemag.org/cgi/collection/anatmorp},
  urldate = {1998-09-23}
}

@online{Xiao2001,
  author  = {萧钰},
  title   = {出版业信息化迈人快车道},
  date    = {2001-12-19},
  url     = {http://www.creader.com/news/20011219/200112190019.html},
  urldate = {2002-04-15}
}

@article{Yang1999,
  author  = {杨瑞林 and 李力军 and 李玉成},
  title   = {新型低合金高强韧性耐磨钢的研究},
  journal = {钢铁},
  year    = {1999},
  volume  = {34},
  number  = {7},
  pages   = {41--45}
}

@article{Schinstock2000,
  author  = {Schinstock, D. E. and Cuttino, J. F.},
  title   = {Real time kinematic solutions of a non-contacting, three dimensional metrology frame},
  journal = {Precision Engineering},
  year    = {2000},
  volume  = {24},
  number  = {1},
  pages   = {70--76}
}

@book{Wen1990,
  author    = {温诗铸},
  title     = {摩擦学原理},
  publisher = {清华大学出版社},
  address   = {北京},
  year      = {1990},
  pages     = {296--300}
}

@standard{GBT16159,
  author    = {国家技术监督局},
  title     = {GB/T 16159-1996 汉语拼音正词法基本规则},
  publisher = {中国标准出版社},
  address   = {北京},
  year      = {1996}
}

@article{Tang2023,
   abstract = {Enabling large language models to utilize real-world tools effectively is crucial for achieving embodied intelligence. Existing approaches to tool learning have either primarily relied on extremely large language models, such as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or utilized supervised learning to train limited scopes of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a diverse tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first automatically creates a highly diversified tool-use corpus by building a multi-agent simulation environment. The corpus contains 3938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subsequently, the constructed corpus is employed to fine-tune compact language models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the ability of these models to utilize previously unseen tools without specific training. Experimental results demonstrate that ToolAlpaca achieves effective generalized tool-use capabilities comparable to those of extremely large language models like GPT-3.5, demonstrating that learning generalized tool-use ability is feasible for compact language models.},
   author = {Qiaoyu Tang and Ziliang Deng and Hongyu Lin and Xianpei Han and Qiao Liang and Boxi Cao and Le Sun},
   month = {6},
   title = {ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases},
   url = {https://arxiv.org/abs/2306.05301v2},
   year = {2023},
}

@article{Qin2023,
   abstract = {Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.},
   author = {Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Lauren Hong and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
   month = {7},
   title = {ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs},
   year = {2023},
}

@misc{Li2023,
   abstract = {Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question.},
   author = {Minghao Li and Yingxiu Zhao and Bowen Yu and Feifan Song and Hangyu Li and Haiyang Yu and Zhoujun Li and Fei Huang and Yongbin Li},
   doi = {10.48550/arXiv.2304.08244},
   keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
   month = {10},
   note = {Comment: EMNLP 2023
arXiv:2304.08244 [cs]},
   publisher = {arXiv},
   title = {API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs},
   url = {http://arxiv.org/abs/2304.08244},
   year = {2023},
}


@Comment{jabref-meta: databaseType:biblatex;}
